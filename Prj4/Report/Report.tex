\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{multicol}
\usepackage{multirow}
\usepackage{diagbox}
\usepackage{subfigure}
\usepackage{tabularx}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{float}
\usepackage{xcolor}
\usepackage{indentfirst}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}
\bibliographystyle{IEEEtran}

\title{Principles of Data Science Project 4\\
Domain Adaptation}

\author{\IEEEauthorblockN{Hongzhou Liu}
\IEEEauthorblockA{517030910214}
\texttt{deanlhz@sjtu.edu.cn}
\and
\IEEEauthorblockN{Xuanrui Hong}
\IEEEauthorblockA{517030910227}
\texttt{hongxuanrui.1999@sjtu.edu.cn}
\and
\IEEEauthorblockN{Qilin Chen}
\IEEEauthorblockA{517030910155}
\texttt{1017856853@sjtu.edu.cn}
}

\maketitle

\begin{abstract}
In this project, we tried different domain adaptation methods on the Office-Home dataset, which contains 65 categories of things from 4 domains.
The four domains are Art, Clipart, Product and Real-World. In our experiments, we take Art, Clipart and Product as source domains and Real-World as target domain.
For traditional methods, we tried KMM, CORAL, GFK, TCA, and EasyTL. For deep learning methods, we only tried DAN due to the scarce of computation resources and time limitation.
We compared performances among those methods and discussed the difference among them. 
\end{abstract}

\begin{IEEEkeywords}
Domain Adaptation, Transfer Learning
\end{IEEEkeywords}

\section{Introduction}

In this project, we tried different unsupervised domain adaptation methods on the Office-Home dataset, which contains 65 categories of things from 4 domains. The four domains are Art, Clipart, Product and Real-World. There are two parts in this section. Firstly, we will introduce several traditional transfer learning
methods we used in our project, including KMM, CORAL, GFK TCA and EasyTL. Then we will introduce deep transfer learning method DAN to compare with the traditional transfer learning methods.

\subsection{Traditional Transfer Learning Methods}
\subsubsection{Kernel Mean Matching (KMM)}
Huang et al. proposed Kernel Mean Matching\cite{gretton2009covariate} to estimate the probability density of data samples. The ultimate goal is to weight the samples to make the probability distribution of sourch domain and target domain closer. The core of the method is the measurement of the difference in the distribution of the two areas. Specifically, the weighted data of the two areas is mapped into the RKHS, and the difference between the average values of the samples in each area is obtained. This distribution measurement method is called the maximum mean difference. After learning the weights of samples with similar distributions, standard machine learning algorithms can be trained and predicted. For example, using SVM:
\begin{equation}
step1: \min\limits_{\beta_i}\left\|\frac{1}{n_s}\sum_{i=1}^{n_s}\beta_{i}\phi(x_i^s)-\frac{1}{n_t}\sum_{i=1}^{n_t}\phi(x_i^t)\right\|^2
\end{equation}
\begin{equation}
step2: \min\limits_{w,b,\xi_i}\frac{1}{2}\left\|w\right\|^2+C\sum_i\beta_i\xi_i
\end{equation}
\begin{equation}
s.t. y_i(w^T\phi(x_i)+b)\ge 1-\xi_i, \xi_i\ge 0, \forall i
\end{equation}
\subsubsection{CORrelation ALignment (CORAL)}
The goal of CORAL is to minimize the second-order distance between source domain and target domain, that is, covariance. It linearly transforms the second-order statistics of source distribution and target distribution. Although it is very simple, it works well for unsupervised areas. However, it relies on linear transformation, not end-to-end: it needs to first extract features, apply the transformation, and then train the SVM classifier in a separate step.\par
The main formula is as follows:
\begin{equation}
\min_A\left\|\widehat{C_s}-C_t\right\|^2_F=\min_A\left\|A^TC_sA-C_t\right\|^2_F
\end{equation}
\begin{equation}
C_s = U_s\Sigma_sU_s^T,C_t = U_t\Sigma_tU_t^T
\end{equation}\par
The optimal solution:
\begin{equation} A^*=U_s\Sigma_s^{-\frac{1}{2}}U_s^TU_t[1:r]\Sigma_t[1:r]^{\frac{1}{2}}U_t[1:r]^T
\end{equation}
\begin{equation}
r = min(rank(C_s),rank(C_t))
\end{equation}
\subsubsection{Geodesic Flow Kernel (GFK)}
Geodestic Flow Kernel's\cite{gong2012geodesic} main idea based on the Domain Adaption method of geodetic flow cores (preferably displayed in color). They embed the source data set and the target data set into the Glassman manifold. Then, they construct a flow between two points of the geodesic line and integrate the infinite flow $\Phi(t)$ in the subspace. Specifically, the original features are projected into these subspaces to form an infinite dimensional feature vector $z_h$. The inner product between these feature vectors defines a kernel function that can be calculated in a closed form on the original feature space. The kernel encapsulates incremental changes between subspaces, and these changes are the basis for the differences and commonalities between the two domains. Therefore, the learning algorithm uses this kernel to derive a low-dimensional representation of domain invariance.
The main idea is shown in fig .\ref{fig:gfk}.
\begin{center}
	\begin{figure}[htbp]
		\centering
		\includegraphics[width=5cm]{img/gfk.png}
		\caption{}
	\end{figure}\label{fig:gfk}
\end{center}

\subsubsection{Transfer Component Analysis (TCA)}

For domain adaptation, Transfer Component Analysis (TCA) \cite{Sinno2011Domain} tries to learn some transfer components across domains in a Reproducing Kernel Hilbert Space (RKHS) using Maximum Mean Discrepancy (MMD). It minimizes the distance between domain distributions by projecting data onto the learned transfer components.

The basic assumption of TCA is 
$$P\left(X_{s}\right) \neq P\left(X_{t}\right)$$
where $X_{s}$ denotes source domain data and $P\left(X_{s}\right)$ denotes its marginal distributions, $X_{t}$ denotes target domain data and $P\left(X_{t}\right)$ denotes its marginal distributions. The motivation of TCA is to find a map $\Phi$ which could preserve the most data properties after projection, which means obtain the most variance, i.e.
$$P\left(\phi\left(\mathbf{x}_{s}\right)\right) \approx P\left(\phi\left(\mathbf{x}_{t}\right)\right)$$
or we can find conditional distribution of the two will also be similar as:
$$
\left.\left.P\left(y_{s} \mid \phi\left(\mathbf{x}_{s}\right)\right)\right) \approx P\left(y_{t} \mid \phi\left(\mathbf{x}_{t}\right)\right)\right)
$$
We ccan give the maximum mean discrepancy (MMD) formula as:
$$
M M D(X, Y)=\left\|\frac{1}{n_{1}} \sum_{i=1}^{n_{1}} \Phi\left(x_{i}\right)-\frac{1}{n_{2}} \sum_{j=1}^{n_{2}} \Phi\left(y_{j}\right)\right\|^{2}
$$
where $n_1$, $n_2$ are the number of instances of the two domains. Then by changing the solution of this function to the solution of the kernel function, we can get: 
$$
\operatorname{Dist}\left(X_{S}^{\prime}, X_{T}^{\prime}\right)=\operatorname{tr}(K L)
$$
where
$$
K=\left[\begin{array}{ll}
K_{S, S} & K_{S, T} \\
K_{T, S} & K_{T, T}
\end{array}\right] \in \mathbb{R}^{\left(n_{1}+n_{2}\right) \times\left(n_{1}+n_{2}\right)}
$$,
then, Q Yang\cite{Sinno2011Domain} decomposed $K$ to transfer this problem to:
$$
\left\{\begin{array}{l}
\min t r\left(W^{T} K L K W\right)+\mu t r\left(W^{T} W\right) \\
\text { s.t. } W^{T} K H K W=I_{m}
\end{array}\right.
$$
Finally, we can get the solution $W*$ as the $m$ leading eigenvectors of
$$
(K L K+\mu I)^{-1} K H K
$$

\subsubsection{Easy Transfer Learning (EasyTL)}

Most traditional and deep learning migration algorithms are parametric methods, which require a lot of time and money to train those hyperparameters. In order to overcome these drawbacks, Easy Transfer Learning (EasyTL) \cite{Wang2019Easy} learns non-parametric transfer features through intra-domain alignment, and learns transmission classification through intra-domain programming. EasyTL can also improve the performance of existing TL methods through in-domain programming as the final classifier, the procedure of EasyTL can be shown in Fig. \ref{kFig1}. 

\begin{center}
	\begin{figure}[htbp]
		\centering
		\label{kFig1}
		\includegraphics[width=8cm]{image/easyTL.png}
		\caption{procedure of EasyTL\cite{Wang2019Easy}}
	\end{figure}
\end{center}

Instead of learning $\mathbf{y}^{t}$ directly, Easy Transfer Learning algorithm focuses on learning the probability annotation matrix $\mathbf{M}$. In this way, the cost function can be formalized as:
$$
\mathcal{J}=\sum_{j}^{n_{t}} \sum_{c}^{C} D_{c j} M_{c j}
$$
where the distance value $D_{c j}$ is an entry in a distance matrix
D. $D_{c j}$ denotes the distance between $\mathbf{x}_{j}^{t}$ and the $c$ -th class center of the source domain $\Omega_{s}^{(c)}$.

We can use constrains optimazition to get the softmax function, which give the label estimation value:
$$
y_{j}^{t}=\underset{r}{\arg \max } \frac{M_{r j}}{\sum_{c}^{C} M_{c j}} \text { for } r \in\{1, \cdots, C\}
$$

It is noticeable that this classifier does not involve any param- eters to tune explicitly. This is significantly different from well-established classifiers such as SVM that needs to tune numerous hyperparameters. In fact, intra-domain programming can be used alone for TL problems.

\subsection{Deep Transfer Learning Methods}

\subsubsection{Deep Adaptation Network (DAN)}

Recent research shows that deep neural networks can learn transferable features, which can be well extended to new fields to adapt to tasks. Deep Adaptation Network (DAN) use deep net to optimize the loss function and distribution distance in Regenerative Nuclear Hilbert Space (RKHS) \cite{Long2015Learning}.

Denote $\mathcal{H}_{k}$ as the reproducing kernel Hilbert space (RKHS) endowed with a characteristic kernel $k$. The average embedding of the distribution $p$ in $\mathcal{H}_{k}$ is a unique element $k(p)$, making $\mathbf{E}_{\mathbf{x} \sim p} f(\mathbf{x})=\left\langle f( \mathbf{x}), \mu_{k}(p)\right\rangle_{\mathcal{H}_{k}}$ for all $f \in \mathcal{H}_{k}$. Define the MK-MMD $d_{k}(p, q)$between the probability distributions $p$ and $q$ as the average embedding distance RKHS of $p$ and $q$, and define the square formula of MK-MMD as

$$
d_{k}^{2}(p, q) \triangleq\left\|\mathbf{E}_{p}\left[\phi\left(\mathbf{x}^{s}\right)\right]-\mathbf{E}_{q}\left[\phi\left(\mathbf{x}^{t}\right)\right]\right\|_{\mathcal{H}_{k}}^{2}
$$
and the kernel defined by the multiple cores is
$$
\mathcal{K} \triangleq\left\{k=\sum_{u=1}^{m} \beta_{u} k_{u}: \sum_{u=1}^{m} \beta_{u}=1, \beta_{u} \geqslant 0, \forall u\right\}
$$

Global optimization goal consists of two parts: loss function and distribution distance. The loss function is used to measure the difference between the predicted value and the true value.

DAN use adaptive method based on mk-mmd and CNNs to onercome that the target domain has no or only limited label information, so it is impossible to adapt CNN directly to the target domain through fine-tuning, or it is easy to overfit. Fig. \ref{kFig2} gives a description of the proposed DAN model.

\begin{center}
	\begin{figure}[htbp]
		\centering
		\label{kFig2}
		\includegraphics[width=8cm]{image/DAN.png}
		\caption{The DAN architecture for learning transferable features.Since deep features eventually transition from general to specific along the network, (1) the features extracted by convolutional layers conv1–conv3 are general, hence these layers are frozen, (2) the features extracted by layers conv4–conv5 are slightly less transferable, hence these layers are learned via fine-tuning, and (3) fully connected layers fc6–fc8 are tailored to fit specific tasks, hence they are not transferable and should be adapted with MK-MMD.\cite{Long2015Learning}}
	\end{figure}
\end{center}

DAN fine-tuned the source of the labeled examples, requiring that under the hidden representation of the fully connected layers $f$ $c6$ $f$ $c8$, the distribution of the source and target becomes similar. This can be achieved by adding a multi-layer adaptive regularizer (1) based on mk-mmd to the risk (3) of CNN:

$$
\min _{\Theta} \frac{1}{n_{a}} \sum_{i=1}^{n_{a}} J\left(\theta\left(\mathbf{x}_{i}^{a}\right), y_{i}^{a}\right)+\lambda \sum_{\ell=l_{1}}^{l_{2}} d_{k}^{2}\left(\mathcal{D}_{s}^{\ell}, \mathcal{D}_{t}^{\ell}\right)
$$


where $\lambda > 0$ is a penalty parameter, $l_1$ and $l_2$ are layer indices between which the regularizer is effective.

\section{Experiments}

In this part, we will show the experimental results and comparative analysis of the results through two types of traditional transfer learning methods and deep transfer learning methods.


\subsection{Baseline}
Before using transfer learning methods, we applied linear SVM and rbf kernel SVM directly and used the results as baseline of this experiment. Besides, we used $GridSearchCV$ to find the optimal value of parameters. The results are shown in Table .\ref{tab:base}. And we visualize the source domain and target domain, the result is shown in the fig. \ref{fig:base}.

\begin{table}[htbp]
	\centering
	\caption{the result of baseline}
\begin{tabular}{|l|c|c|c|}
	\hline
	\diagbox{dataset}{result} & accuracy & C & kernel \\
	\hline
	Art->RealWorld & 0.7484  & 10.0 & rbf \\
	\hline
	Clipart->RealWorld & 0.6577 & 0.01 & linear \\
	\hline
	Product->RealWorld & 0.7294 & 10.0 & rbf \\
	\hline
\end{tabular}\label{tab:base}
\end{table}

\begin{center}
	\begin{figure}[htbp]
		\centering
		\subfigure[A->RealWorld]{
			\includegraphics[width=2.3cm]{img/A_R_baseline.png}
		}
		\quad
		\subfigure[C->RealWorld]{
			\includegraphics[width=2.3cm]{img/C_R_baseline.png}
		}
		\quad
		\subfigure[P->RealWorld]{
			\includegraphics[width=2.3cm]{img/P_R_baseline.png}
		}
		\caption{baseline}
		\label{fig:base}
	\end{figure}
\end{center}

\subsection{Traditional Transfer Learning Methods}
\subsubsection{KMM}
In this experiment, we used the KMM method to assign different weights on source domain samples in order to make the probability distribution of the weighted source and target domains as close as possible. And our experiment results are shown in Table \ref{tab:kmm}.\par
From the results, we can see that rbf and linear KMM perform similarly, and there is almost no improvement compared to baseline. So we visualized the result of KMM for Art-Realworld in fig .\ref{fig:kmm}.We speculate that the reason why the experimental performance has hardly improved is that the weights given by KMM to most samples are similar, which may not help to narrow the sample distribution of the source domain and target domain. In addition, the sample distribution of the source domain and target domain is not particularly different, and may be one of the reasons.
\begin{table}[H]
	\centering
	\caption{the result of KMM}
	\begin{tabular}{|l|c|c|}
		\hline
		\diagbox{dataset}{result} & accuracy  & kernel \\
		\hline
		Art->RealWorld & \textbf{0.7528} & linear \\
		\hline
		Clipart->RealWorld & 0.6518 & linear \\
		\hline
		Product->RealWorld & 0.7296 & linear \\
		\hline
		Art->RealWorld & \textbf{0.7528}  & rbf \\
		\hline
		Clipart->RealWorld & 0.6513 & rbf \\
		\hline
		Product->RealWorld & 0.7294 & rbf \\
		\hline
	\end{tabular}\label{tab:kmm}
\end{table}

\begin{center}
	\begin{figure}[H]
		\centering
		\includegraphics[width=5cm]{img/A_R_weight_rbf.png}
		\caption{Samlpe weights of Art-Realword of RBF}
	\end{figure}\label{fig:kmm}
\end{center}

\subsubsection{CORAL}
In this experiment, we used the CORAL method to minimize the second-order distance between source domain and target domain. And our experiment results are shown in Table \ref{tab:CORAL}.\par
From the results we can see that only the experiment results of Product-Realworld are slightly better than the baseline. We visualize the source domain processed by the CORAL method and compare it with the baseline. We can't find that the obvious data distribution is getting closer. We speculate that the reason CORAL is not performing well is that it is not suitable for no-deep models. In addition, we checked the dataset and found that there are few pictures in some categories, which may lead to insufficient training of the model and affect the accuracy of the test. Hoffman J and others proposed the deep CORAL method based on CORAL\cite{tzeng2017adversarial}, which uses the CORAL loss function for neural network training, which greatly improves the experimental results. Unfortunately, due to limited computing resources, we did not use this method for experiment.
\begin{table}[H]
	\centering
	\caption{the result of CORAL}
\begin{tabular}{|l|c|}
	\hline
	\diagbox{dataset}{result} & accuracy \\
	\hline
	Art->RealWorld & 0.7381 \\
	\hline
	Clipart->RealWorld & 0.6513 \\
	\hline
	Product->RealWorld & 0.7369 \\
	\hline
\end{tabular}\label{tab:CORAL}
\end{table}

\begin{center}
	\begin{figure}[H]
		\centering
		\subfigure[baseline]{
			\includegraphics[width=3cm]{img/P_R_baseline.png}
		}
		\quad
		\subfigure[CORAL]{
			\includegraphics[width=3cm]{img/P_R_CORAL.png}
		}
		\caption{baseline and CORAL for Product-Realworld}
		\label{fig:CORAL}
	\end{figure}
\end{center}

\subsubsection{GFK}
In this experiment we changed the dimension of geodesic flow kernel for comparative experiment. The experimental results are shown in the Table \ref{tab:GFK}.\par
From the results we can see that the dimension of geodesic flow kernel will affect the experiment, and the optimal dimension in this project is 128, that is, the source domain and target domain can maintain maximum consistency in the 128-dimensional subspace. We speculate that when d is too small, the subspace cannot maintain the variance of the source domain to construct a good classifier. But when d is too large, the two subspaces start to appear in orthogonal directions, which will destroy the experimental results. For the three data sets, GFK's experimental results have improved compared to the baseline. We visualize the source domain and target domain after GFK processing, and compare with the baseline, as shown in the Fig .\ref{fig:GFK}. We can find that GFK makes the sample distribution of source domain and target domain indeed closer.
\begin{table}[H]
	\centering
	\caption{the result of GFK}
	\begin{tabular}{|l|c|c|}
		\hline
		\diagbox{dataset}{result} & accuracy & dimension \\
		\hline
		Art->RealWorld & 0.7190 & \multirow{3}*{32}\\
		\cline{1-2}
		Clipart->RealWorld & 0.6336 & \\
		\cline{1-2}
		Product->RealWorld & 0.7117 &\\
		\hline
		Art->RealWorld & 0.7456 & \multirow{3}*{64}\\
		\cline{1-2}
		Clipart->RealWorld & 0.6561 & \\
		\cline{1-2}
		Product->RealWorld & \textbf{0.7335} &\\
		\hline
		Art->RealWorld & \textbf{0.7537} & \multirow{3}*{128}\\
		\cline{1-2}
		Clipart->RealWorld & \textbf{0.6598} & \\
		\cline{1-2}
		Product->RealWorld & 0.7310 &\\
		\hline
		Art->RealWorld & 0.7482 & \multirow{3}*{256}\\
		\cline{1-2}
		Clipart->RealWorld & 0.6566 & \\
		\cline{1-2}
		Product->RealWorld & 0.7300 &\\
		\hline
		Art->RealWorld & 0.7475 & \multirow{3}*{512}\\
		\cline{1-2}
		Clipart->RealWorld & 0.6575 & \\
		\cline{1-2}
		Product->RealWorld & 0.7284 &\\
		\hline
		Art->RealWorld & 0.7475 & \multirow{3}*{1024}\\
		\cline{1-2}
		Clipart->RealWorld & 0.6580 & \\
		\cline{1-2}
		Product->RealWorld & 0.7296 &\\
		\hline
	\end{tabular}\label{tab:GFK}
\end{table}

\begin{center}
	\begin{figure}[H]
		\centering
		\subfigure[baseline]{
			\includegraphics[width=3cm]{img/A_R_baseline.png}
		}
		\quad
		\subfigure[GFK(128)]{
			\includegraphics[width=3cm]{img/A_R_GFK_128.png}
		}
		\caption{baseline and GFK for Art-Realworld}
		\label{fig:GFK}
	\end{figure}
\end{center}

\begin{center}
	\begin{figure}
		\centering
		\subfigure[Art, RealWorld distribution before TCA]{
			\includegraphics[width=3.5cm]{image/A_R_baseline.png}
		}
		\quad
		\subfigure[Art, RealWorld distribution after TCA]{
			\includegraphics[width=3.5cm]{image/A_R_TCA_primal_256.png}
		}
		\quad
		\subfigure[Clipart, RealWorld distribution after TCA]{
			\includegraphics[width=3.5cm]{image/C_R_baseline.png}
		}
		\quad
		\subfigure[Clipart, RealWorld distribution after TCA]{
			\includegraphics[width=3.5cm]{image/C_R_TCA_primal_256.png}
		}
		\quad
		\subfigure[Product, RealWorld distribution after TCA]{
			\includegraphics[width=3.5cm]{image/P_R_baseline.png}
		}
		\quad
		\subfigure[Product, RealWorld distribution after TCA]{
			\includegraphics[width=3.5cm]{image/P_R_TCA_primal_128.png}
		}
		\caption{TCA and baselines}
		\label{kFig3}
	\end{figure}
\end{center}

\begin{table}[htbp]
\caption{Accuracy of TCA-based Method}
\label{kbase1}%
\begin{tabular}{@{}p{0.3cm}<{\centering}|l|l|l|l|l|l|l|l|l|l|}
\hline
Task & \multicolumn{3}{l|}{Art-\textgreater{}RealWorld} & \multicolumn{3}{l|}{Clipart-\textgreater{}RealWorld} & \multicolumn{3}{l|}{Product-\textgreater{}RealWorld} \\ \hline
Kn   & primal          & linear         & rbf           & primal           & linear           & rbf            & primal           & linear           & rbf            \\ \hline
32       & 71.59           & 69.82          & 70.74         & 64.45            & 62.61            & 63.44          & 71.29            & 69.34            & 70.69          \\ \hline
64       & 75.28           & 72.89          & 73.74         & 65.92            & 63.71            & 65.32          & 73.35            & 71.33            & 72.78          \\ \hline
128      & 75.49           & 73.28          & 74.27         & 65.32            & 63.78            & 65.27          & 73.67            & 71.56            & 73.24          \\ \hline
256      & 75.74           & 73.40          & 74.18         & 66.12            & 63.85            & 65.23          & 73.63            & 71.70            & 73.24          \\ \hline
512      & 75.58           & 73.40          & 74.27         & 66.12            & 63.81            & 65.25          & 73.56            & 71.70            & 73.12          \\ \hline
1024     & 75.53           & 73.40          & 74.23         & 66.10            & 63.81            & 65.25          & 73.63            & 71.70            & 73.16          \\ \hline
2048     & 75.60           & 73.40          & 74.27         & 66.08            & 63.81            & 65.25          & 73.65            & 71.70            & 73.14          \\ \hline
\end{tabular}
\end{table}

\subsubsection{Transfer Component Analysis (TCA)}

In this experiment, we test TCA method on kernels amount $[primal, linear, rbf]$, and various aim dimension in $[32,64,128,256,512,1024,2048]$, and different domain transfer tasks in [Art->RealWorld, Clipart->RealWorld, Product->RealWorld]. We can show our experimental results in Tab. \ref{kbase1} . To figure out why TCA can have better performance, we plot the source domain data and target domain data in the same figure. Here, we can see the data distribution of case A-R, C-R and P-R in Fig. \ref{kFig3}.

Unlike the sample-based transfer learning method like KMM, TCA is a
feature-based transfer learning method. As we can see from Fig. \ref{kFig3}, after using TCA, the area where the source domain data distribution coincides with the target domain data distribution is larger. And it's clear that TCA has different degrees of improvement for the accuracy of the three cases (A-R, C-R, P-R) and TCA performance is better than KMM performance.


\subsubsection{Easy Transfer Learning (EasyTL)}

In the previous experiments, we find that the model performs good need extensive parameter tuing and a large mount of time to fix or train. With regard to Easy Transfer Learning (EasyTL), it's proved to maintain good performance while avoiding extensive parameter tuning. According to previous related work, there are four kinds of methods for Intra-domain alignment, they are RAW(do not use Intra-domain alignment), PCA, CORAL, GFK. We test Raw and Coral in this experiment.

In this experiment, we test EasyTL method on intra\_align amount $[raw, coral]$, and various dist metrics in $[euclidean,ma,cosine,rbf]$, and different domain transfer tasks in [Art->RealWorld, Clipart->RealWorld, Product->RealWorld]. We can show our experimental results in Tab. \ref{tab:ETL}. 

\begin{table}[htbp]
	\centering
	\caption{the result of EasyTL}
\begin{tabular}{|l|c|c|c|}
	\hline
	\diagbox{dataset}{result} & accuracy & dist & intra\_align \\
	\hline
	Art->RealWorld & 0.7448  & euclidean & raw \\
	\hline
	Art->RealWorld & 0.7565  & euclidean & coral \\
	\hline
	Art->RealWorld & 0.0592  & ma & raw \\
	\hline
	Art->RealWorld & 0.0566  & ma & coral \\
	\hline
	Art->RealWorld & 0.7372  & cosine & raw \\
	\hline
	Art->RealWorld & 0.7539  & cosine & coral \\
	\hline
	Art->RealWorld & 0.0153  & rbf & raw \\
	\hline
	Art->RealWorld & 0.0153  & rbf & coral \\
	\hline
	Clipart->RealWorld & 0.6520 & euclidean & raw \\
	\hline
	Clipart->RealWorld & 0.6768 & euclidean & coral \\
	\hline
	Clipart->RealWorld & 0.0433 & ma & raw \\
	\hline
	Clipart->RealWorld & 0.0527 & ma & coral \\
	\hline
	Clipart->RealWorld & 0.6628 & cosine & raw \\
	\hline
	Clipart->RealWorld & 0.6823 & cosine & coral \\
	\hline
	Clipart->RealWorld & 0.0146 & rbf & raw \\
	\hline
	Clipart->RealWorld & 0.0146 & rbf & coral \\
	\hline
	Product->RealWorld & 0.7378 & euclidean & raw \\
	\hline
	Product->RealWorld & 0.7514 & euclidean & coral \\
	\hline
	Product->RealWorld & 0.0658 & ma & raw \\
	\hline
	Product->RealWorld & 0.0530 & ma & coral \\
	\hline
	Product->RealWorld & 0.7376 & cosine & raw \\
	\hline
	Product->RealWorld & 0.7502 & cosine & coral \\
	\hline
	Product->RealWorld & 0.0174 & rbf & raw \\
	\hline
	Product->RealWorld & 0.0174 & rbf & coral \\
	\hline
\end{tabular}\label{tab:ETL}
\end{table}

We can find with ma and rbf distance metrics the EasyTL performs poor, and we deem the reason that EasyTL can't performs well with complicated model like rbf. And we can find with coral intra\_align, EasyTL performs better than raw one. Compared with othor methods in our work, we can find EasyTL have relatively better performance while cost less time, we deem the reason that EasyTL obtains the softmax probability (floating point weight) through linear programming. In this way, EasyTL not only considers the relationship between the sample and the center, but also considers the relationship with other samples. 

\begin{center}
	\begin{figure}
		\centering
		\subfigure[A-R loss via DAN]{
			\includegraphics[width=2.2cm]{image/ARLoss.png}
		}
		\quad
		\subfigure[P-R loss via DAN]{
			\includegraphics[width=2.2cm]{image/ARLoss.png}
		}
		\quad
		\subfigure[C-R loss via DAN]{
			\includegraphics[width=2.2cm]{image/CRLoss.png}
		}
		\caption{Loss tendency in DAN}
		\label{kFig4}
	\end{figure}
\end{center}

\begin{center}
	\begin{figure}
		\centering
		\subfigure[Accuracy comprison via DAN]{
			\includegraphics[width=8cm]{image/Acc.png}
		}
		\caption{Accuracy tendency in DAN}
		\label{kFig5}
	\end{figure}
\end{center}

\subsection{Deep Transfer Learning Methods}

\subsubsection{Deep Adaptation Network (DAN)}

In this experiment, we test DAN method via different domain transfer tasks in [Art->RealWorld, Clipart->RealWorld, Product->RealWorld]. We can show our experimental results in Tab. \ref{tab9} . To figure out The loss and accuracy of the training process, we can see them in Fig. \ref{kFig4} and Fig. \ref{kFig5}. 

\begin{table}[H]
	\centering
	\caption{the result of KMM}
	\begin{tabular}{|l|c|}
		\hline
		\diagbox{dataset}{result} & accuracy(\%) \\
		\hline
		Art->RealWorld & 75.63\\
		\hline
		Clipart->RealWorld & 67.68 \\
		\hline
		Product->RealWorld & 75.14 \\
		\hline
	\end{tabular}\label{tab9}
\end{table}

We know that with regard to DAN, global optimization goal consists of two parts: loss function and distribution distance. The loss function is used to measure the difference between the predicted value and the true value. Therefore every iteration makes the distribution between source domain and target domain closer. We can see it in the iteration acuracy tendency, but it's time-consuming and for some case, it performs poor due to the interval between two aim domain.


\section{Conclusion}
In this project, we try totally 6 methods about transfer learning. We can find deep network method DAN achieves best performance and hardly affected by the initial parameters settings. TCA is fast and gives a not bad result. compared with above, EasyTL is a good choice because it can achieve a very good performance without extensive parameters tuning and a lot of training time.

\bibliography{Prj4}
\end{document}