\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{multicol}
\usepackage{multirow}
\usepackage{diagbox}
\usepackage{subfigure}
\usepackage{tabularx}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{indentfirst}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}
\bibliographystyle{IEEEtran}

\title{Principles of Data Science Project 4\\
Domain Adaptation}

\author{\IEEEauthorblockN{Hongzhou Liu}
\IEEEauthorblockA{517030910214}
\texttt{deanlhz@sjtu.edu.cn}
\and
\IEEEauthorblockN{Xuanrui Hong}
\IEEEauthorblockA{517030910227}
\texttt{hongxuanrui.1999@sjtu.edu.cn}
\and
\IEEEauthorblockN{Qilin Chen}
\IEEEauthorblockA{517030910155}
\texttt{1017856853@sjtu.edu.cn}
}

\maketitle

\begin{abstract}
In this project, we tried different domain adaptation methods on the Office-Home dataset, which contains 65 categories of things from 4 domains.
The four domains are Art, Clipart, Product and Real-World. In our experiments, we take Art, Clipart and Product as source domains and Real-World as target domain.
For traditional methods, we tried KMM, CORAL, GFK, TCA, and EasyTL. For deep learning methods, we only tried DAN due to the scarce of computation resources and time limitation.
We compared performances among those methods and discussed the difference among them. 
\end{abstract}

\begin{IEEEkeywords}
Domain Adaptation, Transfer Learning
\end{IEEEkeywords}

\section{Introduction}

In this project, we tried different unsupervised domain adaptation methods on the Office-Home dataset, which contains 65 categories of things from 4 domains. The four domains are Art, Clipart, Product and Real-World. There are two parts in this section. Firstly, we will introduce several traditional transfer learning
methods we used in our project, including KMM, CORAL, GFK TCA and EasyTL. Then we will introduce deep transfer learning method DAN to compare with the traditional transfer learning methods.

\subsection{Transfer Component Analysis (TCA)}


For domain adaptation, Transfer Component Analysis (TCA) \cite{Sinno2011Domain} tries to learn some transfer components across domains in a Reproducing Kernel Hilbert Space (RKHS) using Maximum Mean Discrepancy (MMD). It minimizes the distance between domain distributions by projecting data onto the learned transfer components.

The basic assumption of TCA is 
$$P\left(X_{s}\right) \neq P\left(X_{t}\right)$$
where $X_{s}$ denotes source domain data and $P\left(X_{s}\right)$ denotes its marginal distributions, $X_{t}$ denotes target domain data and $P\left(X_{t}\right)$ denotes its marginal distributions. The motivation of TCA is to find a map $\Phi$ which could preserve the most data properties after projection, which means obtain the most variance, i.e.
$$P\left(\phi\left(\mathbf{x}_{s}\right)\right) \approx P\left(\phi\left(\mathbf{x}_{t}\right)\right)$$
or we can find conditional distribution of the two will also be similar as:
$$
\left.\left.P\left(y_{s} \mid \phi\left(\mathbf{x}_{s}\right)\right)\right) \approx P\left(y_{t} \mid \phi\left(\mathbf{x}_{t}\right)\right)\right)
$$
We ccan give the maximum mean discrepancy (MMD) formula as:
$$
M M D(X, Y)=\left\|\frac{1}{n_{1}} \sum_{i=1}^{n_{1}} \Phi\left(x_{i}\right)-\frac{1}{n_{2}} \sum_{j=1}^{n_{2}} \Phi\left(y_{j}\right)\right\|^{2}
$$
where $n_1$, $n_2$ are the number of instances of the two domains. Then by changing the solution of this function to the solution of the kernel function, we can get: 
$$
\operatorname{Dist}\left(X_{S}^{\prime}, X_{T}^{\prime}\right)=\operatorname{tr}(K L)
$$
where
$$
K=\left[\begin{array}{ll}
K_{S, S} & K_{S, T} \\
K_{T, S} & K_{T, T}
\end{array}\right] \in \mathbb{R}^{\left(n_{1}+n_{2}\right) \times\left(n_{1}+n_{2}\right)}
$$,
then, Q Yang\cite{Sinno2011Domain} decomposed $K$ to transfer this problem to:
$$
\left\{\begin{array}{l}
\min t r\left(W^{T} K L K W\right)+\mu t r\left(W^{T} W\right) \\
\text { s.t. } W^{T} K H K W=I_{m}
\end{array}\right.
$$
Finally, we can get the solution $W*$ as the $m$ leading eigenvectors of
$$
(K L K+\mu I)^{-1} K H K
$$

\subsection{Easy Transfer Learning (EasyTL)}

Most traditional and deep learning migration algorithms are parametric methods, which require a lot of time and money to train those hyperparameters. In order to overcome these drawbacks, Easy Transfer Learning (EasyTL) \cite{Wang2019Easy} learns non-parametric transfer features through intra-domain alignment, and learns transmission classification through intra-domain programming. EasyTL can also improve the performance of existing TL methods through in-domain programming as the final classifier, the procedure of EasyTL can be shown in Fig. \ref{kFig1}. 

\begin{center}
	\begin{figure}[htbp]
		\centering
		\label{kFig1}
		\includegraphics[width=7cm]{image/easyTL.png}
		\caption{procedure of EasyTL\cite{Wang2019Easy}}
	\end{figure}
\end{center}


\subsection{Deep Adaptation Network (DAN)}

\section{Experiments}

In this part, we will show the experimental results and comparative analysis of the results through two types of traditional transfer learning methods and deep transfer learning methods.


\subsection{Transfer Component Analysis (TCA)}



\subsection{Easy Transfer Learning (EasyTL)}


\subsection{Deep Adaptation Network (DAN)}

\section{Conclusion}

\cite{lowe1999object}
\bibliography{Prj4}
\end{document}