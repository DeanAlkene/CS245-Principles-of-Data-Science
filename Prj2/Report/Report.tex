\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{multicol}
\usepackage{multirow}
\usepackage{diagbox}
\usepackage{subfigure}
\usepackage{tabularx}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{indentfirst}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}
\bibliographystyle{IEEEtran}

\title{Principles of Data Science Project 2\\
Distance Metrics}

\author{\IEEEauthorblockN{Hongzhou Liu}
\IEEEauthorblockA{517030910214}
\texttt{deanlhz@sjtu.edu.cn}
\and
\IEEEauthorblockN{Xuanrui Hong}
\IEEEauthorblockA{517030910227}
\texttt{hongxuanrui.1999@sjtu.edu.cn}
\and
\IEEEauthorblockN{Qilin Chen}
\IEEEauthorblockA{517030910155}
\texttt{1017856853@sjtu.edu.cn}
}

\maketitle

\begin{abstract}
Hello
\end{abstract}

\begin{IEEEkeywords}
kNN, Distance Metric, Metric Learning
\end{IEEEkeywords}

\section{Introduction}
\subsection{k-Nearest Neighbor}
K-Nearest Neighbor is a kind of supervised learning method. It can be used for both classification and regression.
In the case of classification, the input consists of the $k$ closest training examples in the feature space and the output is a class membership.
A certain object is classified by a plurality vote of its neighbors, with it being assigned to the class which is the most common in its $k$ nearsest neighbors.
The algorithm is non-parametric\cite{knn}, which means the model is distribution free or with a specified distribution whose parameters are unspecified.
It is also a type of instance-based learning or lazy learning, where the generalization of the training data is delayed until a query is made. In this case, $k$-NN has no explicit 
training step and does all computations during testing period. Because we are finding the $k$ nearest neighbors, the distance metric to evaluate "nearest" is significant in $k$-NN.
\subsection{Distance Metrics}
\subsubsection{Minkowski Distance}
\subsubsection{Chebyshev Distance}
\subsubsection{Euclidean Distance}
\subsubsection{Manhattan Distance}
\subsubsection{Cosine Distance}
\subsubsection{Mahalanobis Distance}
\subsection{Metric Learning}
\section{Simple Distance Metrics}
\section{Metric Learning}
\subsection{Information Theoretic Metric Learning (ITML)}
ITML is an information-besed learning approach taht formulate the problem as that of minimizing the differential relative entropy between two multivariate Gaussians under constraints on the distance function\cite{Davis2007Information}. In this section, we will introduce the experiment on pure ITML, PCA-based ITML and LDA-based ITML.

In this experiment, we trained supervised ITML model in training dataset, then transform source data to ITML-learned data for KNN classification. We set $constraints=200$ in supervised ITML model and give the $k-$neighbors range in $k-$NN model as $[2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]$. For PCA-based ITML, we first perprocess the data by 50-conponents linear PCA, then train supervised ITML model on the processed data, it will give a better performance. For LDA-based one, we perprocess the data by 40-conponents LDA, instead.

\begin{table}[htbp]
	\centering
 	\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}
 	\renewcommand\arraystretch{1.0}
 	\caption{Comparison of Variance ITML and baselines in $k-$NN Classification Task}
 	\label{base2}%
 		\begin{tabular}{@{}p{2.5cm}<{\centering}|c|c|c}
 		\hline
 		\multirow{2}{*}{\diagbox[height=2\line,width=2.9cm,font=\tiny]{$\theta$}{Acc.}{$\mathit{M}$}} &
 		\multicolumn{3}{c}{Variance ITML model + $k-$NN}\\
 		\cline{2-4}
 		& {ITML(\%)} & {PCA-based(\%)} & {LDA-based(\%)}\\
 		\hline
 		2   & 82.04 & 83.18 & 88.06\\
 		\hline
 		3   & \textbf{83.60} & 85.41 & 89.57\\
 		\hline
 		4   & 83.31 & 85.55 & 89.42\\
 		\hline
 		5   & 83.52  & 86.19 & 90.01\\
 		\hline
 		6   & 83.33  & 86.21 & 90.04\\
 		\hline
 		7   & 83.37  & \textbf{86.67} & 90.35\\
 		\hline
 		8   & 83.02  & 86.51 & 90.35\\
 		\hline
 		9   & 82.97  & 86.37 & 90.40\\
 		\hline
 		10   & 82.72  & 86.41 & 90.55\\
 		\hline
 		11   & 82.73  & 86.25 & 90.64\\
 		\hline
 		12   & 82.52  & 86.27 & 90.67\\
 		\hline
 		13   & 82.47  & 86.17 & 90.58\\
 		\hline
 		14   & 82.33  & 86.06 & 90.64\\
 		\hline
 		15   & 82.10  & 86.15 & \textbf{90.74}\\
 		\hline
 		16   & 81.96  & 86.01 & 90.69\\
 		% \hline Baseline& \multicolumn{3}{c}{87.61} \\
 		\hline
 	\end{tabular}
\end{table}

Our experiment results are shown in Tab. \ref{base2}. Compared with baseline $87.61\%$ in Euclidean KNN, we can find that simple ITML model and PCA-based ITML model have poor performance, we deem the reason is that ITML minimize the LogDet divergence subject to linear constraints while it does not rely on an eigenvalue computation, and PCA can't differ the classes which is useless for $k-$NN classification. But we can find PCA-based model have better performance than pure ITML model, we think PCA method can have compensation on eigenvalue computation for ITML. When we set $k=15$, we have best performance $90.74\%$, we think LDA method can make the variance high enough between different clusters. 


\subsection{2}

\section{Conclusion}

\bibliography{Prj2}
\end{document}
