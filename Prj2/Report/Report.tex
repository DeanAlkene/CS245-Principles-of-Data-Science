\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{multicol}
\usepackage{multirow}
\usepackage{diagbox}
\usepackage{subfigure}
\usepackage{tabularx}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{indentfirst}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}
\bibliographystyle{IEEEtran}

\title{Principles of Data Science Project 2\\
Distance Metrics}

\author{\IEEEauthorblockN{Hongzhou Liu}
\IEEEauthorblockA{517030910214}
\texttt{deanlhz@sjtu.edu.cn}
\and
\IEEEauthorblockN{Xuanrui Hong}
\IEEEauthorblockA{517030910227}
\texttt{hongxuanrui.1999@sjtu.edu.cn}
\and
\IEEEauthorblockN{Qilin Chen}
\IEEEauthorblockA{517030910155}
\texttt{1017856853@sjtu.edu.cn}
}

\maketitle

\begin{abstract}
Hello
\end{abstract}

\begin{IEEEkeywords}
kNN, Distance Metric, Metric Learning
\end{IEEEkeywords}

\section{Introduction}
\subsection{k-Nearest Neighbor}
K-Nearest Neighbor is a kind of supervised learning method. It can be used for both classification and regression.
In the case of classification, the input consists of the $k$ closest training examples in the feature space and the output is a class membership.
A certain object is classified by a plurality vote of its neighbors, with it being assigned to the class which is the most common in its $k$ nearsest neighbors.
The algorithm is non-parametric\cite{knn}, which means the model is distribution free or with a specified distribution whose parameters are unspecified.
It is also a type of instance-based learning or lazy learning, where the generalization of the training data is delayed until a query is made. In this case, $k$-NN has no explicit 
training step and does all computations during testing period. Because we are finding the $k$ nearest neighbors, the distance metric to evaluate "nearest" is significant in $k$-NN.
\subsection{Distance Metrics}
\subsubsection{Minkowski Distance}
\subsubsection{Chebyshev Distance}
\subsubsection{Euclidean Distance}
\subsubsection{Manhattan Distance}
\subsubsection{Cosine Distance}
\subsubsection{Mahalanobis Distance}
\subsection{Metric Learning}
\section{Simple Distance Metrics}
\section{Metric Learning}
\subsection{Information Theoretic Metric Learning (ITML)}
ITML is an information-besed learning approach taht formulate the problem as that of minimizing the differential relative entropy between two multivariate Gaussians under constraints on the distance function\cite{Davis2007Information}. In this section, we will introduce the experiment on pure ITML, PCA-based ITML and LDA-based ITML.

In this experiment, we trained supervised ITML model in training dataset, then transform source data to ITML-learned data for KNN classification. We set $constraints=200$ in supervised ITML model and give the $k-$neighbors range in $k-$NN model as $[2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]$. For PCA-based ITML, we first perprocess the data by 50-conponents linear PCA, then train supervised ITML model on the processed data, it will give a better performance. For LDA-based one, we perprocess the data by 40-conponents LDA, instead.
\subsection{2}
\bibliography{Prj2}
\end{document}
