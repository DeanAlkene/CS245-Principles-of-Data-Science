\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{multicol}
\usepackage{multirow}
\usepackage{diagbox}
\usepackage{subfigure}
\usepackage{tabularx}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{indentfirst}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}
\bibliographystyle{IEEEtran}

\title{Principles of Data Science Project 2\\
Distance Metrics}

\author{\IEEEauthorblockN{Hongzhou Liu}
\IEEEauthorblockA{517030910214}
\texttt{deanlhz@sjtu.edu.cn}
\and
\IEEEauthorblockN{Xuanrui Hong}
\IEEEauthorblockA{517030910227}
\texttt{hongxuanrui.1999@sjtu.edu.cn}
\and
\IEEEauthorblockN{Qilin Chen}
\IEEEauthorblockA{517030910155}
\texttt{1017856853@sjtu.edu.cn}
}

\maketitle

\begin{abstract}
The distance metric is an important topic in machine learning. A lot of algorithms rely on a suitable distance metric to perform well, especially $k$-NN. The choice of $k$ nearest neighbors is
closely related to the distance metric. In this project, we tried some classical distance metrics and different metric learning methods to learn some new distance metrics then applied them on $k$-NN classifier and compared the differences
on the classification results.
\end{abstract}

\begin{IEEEkeywords}
kNN, Distance Metric, Metric Learning
\end{IEEEkeywords}

\section{Introduction}
\subsection{k-Nearest Neighbor}
K-Nearest Neighbor is a kind of supervised learning method. It can be used for both classification and regression.
In the case of classification, the input consists of the $k$ closest training examples in the feature space and the output is a class membership.
A certain object is classified by a plurality vote of its neighbors, with it being assigned to the class which is the most common in its $k$ nearsest neighbors.
The algorithm is non-parametric\cite{knn}, which means the model is distribution free or with a specified distribution whose parameters are unspecified.
It is also a type of instance-based learning or lazy learning, where the generalization of the training data is delayed until a query is made. In this case, $k$-NN has no explicit 
training step and does all computations during testing period. Because we are finding the $k$ nearest neighbors, the distance metric to evaluate "nearest" is significant in $k$-NN.
\begin{figure}[htbp]
	\centering
	\includegraphics[scale=0.1]{pic/KNN.png}
	\caption{KNN}
	\label{fig:knn}
\end{figure}
\subsection{Distance Metrics}
\subsubsection{Minkowski Distance}
\par
Minkowski Distance is a metric in normed vector space. It is a generalization of the well-known Euclidean distance, the Manhattan distance and the Chebyshev distance.
The Minkowski distance of order $p$ between two points $\mathbf{x}=(x_1,x_2,\cdots,x_n), \mathbf{y}=(y_1,y_2,\cdots,y_n) \in \mathbb{R}^n$ is defined as
\begin{equation}
    d(\mathbf{x}, \mathbf{y})=\left(\sum_{i=1}^{n}\left|x_{i}-y_{i}\right|^{p}\right)^{\frac{1}{p}}
\end{equation}
The following figure \ref{fig:mkd} shows unit circles (the set of all points which are at the unit distance from the center) with different values of $p$.
\begin{figure}[htbp]
	\centering
	\includegraphics[scale=0.3]{pic/minkowski.png}
	\caption{Unit Circles}
	\label{fig:mkd}
\end{figure}
\subsubsection{Chebyshev Distance}
\par
The Chebyshev distance or $L_{\infty}$ metric is a metric defined on a vector space where the distance between two points is the maximum of their differences along any coordinate dimension.\cite{hmds}
It is actually the Minkowski distance when $p\rightarrow\infty$ and thus is defined as:
\begin{equation}
    d_{Chebyshev}(\mathbf{x},\mathbf{y})=\lim_{p\rightarrow\infty}\left(\sum_{i=1}^{n}\left|x_{i}-y_{i}\right|^{p}\right)^{\frac{1}{p}}=\max_{i}(|x_i-y_i|)
\end{equation}
\subsubsection{Euclidean Distance}
\par
The Euclidean distance is the "ordinary" straight-line distance in Euclidean space. It is also a specialization of Minkowski distance where $p=2$. Thus, it is defined as
\begin{equation}
    d_{Euclidean}(\mathbf{x},\mathbf{y})=\sqrt{\sum_{i=1}^n(x_i-y_i)^2}
\end{equation}
\subsubsection{Manhattan Distance}
\par
The Manhattan distance is a metric in which the distance between two points is the sum of the abosolute differences of their Cartesian coordinates. The name alludes to the island of Manhattan, which causes the shortest paths a
car could take between two intersections to have the same length \ref{fig:mah}. As it's the Minkowski distance with $p=1$, it is defined as:
\begin{equation}
    d_{Manhattan}(\mathbf{x},\mathbf{y})=\left|\sum_{i=1}^n(x_i-y_i)\right|
\end{equation}
\begin{figure}[htbp]
	\centering
	\includegraphics[scale=0.3]{pic/manhattan.png}
	\caption{Manhattan vs Euclidean}
	\label{fig:mah}
\end{figure}
\subsubsection{Cosine Distance}
\par
The Cosine distance is closely related to Cosine similarity where the more similar two points are, the closer their distance is. The Cosine similarity between two non-zero points of an inner product space is defined to equal to the cosine of
the angle between them. Then, we can define it as:
\begin{equation}
    similarity(\mathbf{x},\mathbf{y})=\dfrac{\langle\mathbf{x},\mathbf{y}\rangle}{\|\mathbf{x}\| \|\mathbf{y}\|}=\dfrac{\sum_{i=1}^n x_i y_i}{\sqrt{\sum_{i=1}^n x_i^2}\sqrt{\sum_{i=1}^n y_i^2}}
\end{equation}
Thus, the Cosine distance is defined as:
\begin{equation}
    d_{cosine}(\mathbf{x},\mathbf{y})=1-similarity(\mathbf{x},\mathbf{y})
\end{equation}
\subsubsection{Mahalanobis Distance}
\par
The Mahalanobis distance is a measure of the distance between a point and a distribution. It can be defined as a dissimilarity between two random vectors $\mathbf{x}$ and $\mathbf{y}$ of the same distribution with the covariance matrix $\mathbf{S}$.
\begin{equation}
	d_{Mahalanobis}(\mathbf{x}, \mathbf{y})=\sqrt{(\mathbf{x}-\mathbf{y})^T\mathbf{S}^{-1}(\mathbf{x}-\mathbf{y})}
\end{equation}
In a metric learning task, the aim is to find the covariance matrix $\mathbf{S}$ in the Mahalanobis distance in order to generate a new distance metric. In metric learning, $\mathbf{S}^{-1}$ is always denoted as $\mathbf{M}$ and is a positive semi-definite matrix. As known, a positive semi-definite matrix $\mathbf{M}$ can be 
decomposed as $\mathbf{M}=\mathbf{L}^T\mathbf{L}$ for some $\mathbf{L}$. Thus, the Mahalanobis distance between $\mathbf{x}$ and $\mathbf{y}$ is now:
\begin{equation}
	d_{Mahalanobis}(\mathbf{x}, \mathbf{y})=\sqrt{(\mathbf{L}\mathbf{x}-\mathbf{L}\mathbf{y})^T(\mathbf{L}\mathbf{x}-\mathbf{L}\mathbf{y})}
\end{equation}
It is actually the Euclidean distance after a linear transformation of the feature space defined by $\mathbf{L}$. Thus, metric learning can be seen as learning a new embedding space defined by $\mathbf{L}$. It also reminds us that some learning methods who can learn linear projection matrices can be used here.
In our experiments, we also tried PCA and LDA which can learn the project matrices and also reduce dimensions of the features.
The equivalence of the Mahalanobis distance and the Euclidean distance after linear transformation also helped us accelerating training process. Due to the implementation of \texttt{sklearn}, any self-defined distance metric will have a slower training speed because of the heavy overhead of calling function in Python. By using linear transformation then Euclidean distance,
the training time can be reduced.
\subsection{Metric Learning}
\subsubsection{Information Theoretic Metric Learning}
\par
Information Theoretic Metric Learning (ITML) \cite{Davis2007Information} is a kind of weakly supervised metric learning method. It minimizes the Kullback-Leibler divergence between two multivariate Gaussians subject to constraints on the associated Mahalanobis distance, which can be formulated into a Bregman optimization problem by minimizing the LogDet divergence subject to linear constraints.
The multivariate Gaussian distribution associated with the Mahalanobis distance is
\begin{equation}
	p(\mathbf{x;A})=\dfrac{1}{Z}\exp(-\dfrac{1}{2}(\mathbf{x}-\mu)^T\mathbf{A}(\mathbf{x}-\mu))
\end{equation}
where the inverse of Mahalanobis matrix $\mathbf{A}^{-1}$ is the covariance matrix of the Gaussian.
Then, given pairs of similar points $S$ and dissimilar points $D$, the learning problem is to minimize the LogDet divergence $D_{\ell \mathrm{d}}\left(\mathbf{A}, \mathbf{A}_{0}\right)$, which is equivalent as minimizing the K-L divergence $\mathbf{KL}(p(\mathbf{x};\mathbf{A}_0)\|p(x(\mathbf{x};\mathbf{A}))$:
\begin{equation}
	\label{eq:itml}
	\begin{array}{c}
		\min _{\mathbf{A}} D_{\ell \mathrm{d}}\left(\mathbf{A}, \mathbf{A}_{0}\right)=\operatorname{tr}\left(\mathbf{A} \mathbf{A}_{0}^{-1}\right)-\log \operatorname{det}\left(\mathbf{A} \mathbf{A}_{0}^{-1}\right)-n \\
		\text { subject to } \qquad d_{\mathbf{A}}\left(\mathbf{x}_{i}, \mathbf{x}_{j}\right) \leq u \quad\left(\mathbf{x}_{i}, \mathbf{x}_{j}\right) \in S \\
		\qquad\qquad\qquad\quad d_{\mathbf{A}}\left(\mathbf{x}_{i}, \mathbf{x}_{j}\right) \geq l \quad\left(\mathbf{x}_{i}, \mathbf{x}_{j}\right) \in D
	\end{array}
\end{equation}
As we can see in \ref{eq:itml}, $u$ and $l$ is the upper and lower bound of distance for similar and dissimilar pairs respectively. It means that if the Mahalanobis distance between two points is greater than $l$ then they are considered dissimilar while if the distance is lower than $u$, then they are considered similar.
In addition, the matrix $\mathbf{A}_0$ is the prior distance metric and in practice we always set $\mathbf{A}_0=\mathbf{I}$ to make the prior metric the Euclidean one. So, ITML can optionally incorporate a prior on the distance function and can handle a variety of constraints. It also does not rely on eigenvalue computation or semi-definite programming.
\subsubsection{Local Fisher Discriminant Analysis}
\par
Local Fisher Discriminant Analysis (LFDA) \cite{sugiyama2007dimensionality} is a kind of supervised learning algorithm. It is also a linear dimensionality reduction method. It is particularly useful when dealing with multi-modality, where one ore more classes consist of separate clusters in input space. The core optimization problem of LFDA is solved as a generalized eigenvalue problem.
We need to define the Fisher local within-/between-class scatter matrix $\mathbf{S}^{(w)}$ and $\mathbf{S}^{(b)}$ as
\begin{equation}
	\mathbf{S}^{(w)}=\frac{1}{2} \sum_{i, j=1}^{n} W_{i j}^{(w)}\left(\mathbf{x}_{i}-\mathbf{x}_{j}\right)\left(\mathbf{x}_{i}-\mathbf{x}_{j}\right)^{T}
\end{equation}
\begin{equation}
	\mathbf{S}^{(b)}=\frac{1}{2} \sum_{i, j=1}^{n} W_{i j}^{(b)}\left(\mathbf{x}_{i}-\mathbf{x}_{j}\right)\left(\mathbf{x}_{i}-\mathbf{x}_{j}\right)^{T}
\end{equation}
where
\begin{equation}
	W_{i j}^{(w)}=\left\{\begin{array}{rl}
		0 & y_{i} \neq y_{j} \\
		\mathbf{A}_{i, j} / n_{l} & y_{i}=y_{j}
		\end{array}\right.
\end{equation}
\begin{equation}
	W_{i j}^{(b)}=\left\{\begin{array}{rl}
		1 / n & y_{i} \neq y_{j} \\
		\mathbf{A}_{i, j}\left(1 / n-1 / n_{l}\right) & y_{i}=y_{j}
		\end{array}\right.
\end{equation}
Here, $\mathbf{A}_{i,j}$ is the $(i,j)$-th entry of the affinity matrix $\mathbf{A}$, which can be calculated with local scaling methods. The learning problem then becomes to derive the LFDA transformation matrix $\mathbf{T}_{LFDA}$:
\begin{equation}
	\left.\mathbf{T}_{L F D A}=\arg \max _{\mathbf{T}}\left[\operatorname{tr}\left(\mathbf{T}^{T} \mathbf{S}^{(w)} \mathbf{T}\right)^{-1} \mathbf{T}^{T} \mathbf{S}^{(b)} \mathbf{T}\right)\right]
\end{equation}
That is, it is looking for a transformation matrix $\mathbf{T}$ such that nearby data pairs in the same class are made close and the data pairs in different classes are separated from each other; far apart data pairs in the same class are not imposed to be close.
\subsubsection{Metric Learning with Application for Clustering with Side Information}
\par
Metric Learning with Application for Clustering with Side Information (MMC) \cite{xing2003distance} is also a kind of weakly supervised learning method. It minimizes the sum of squared distances between similar points, while enforcing the sum of distances between dissimilar ones to be greater than one.
It is thus a convex problem and can find local-minima-free optimization solution efficiently. However, the algorithm involves the computation of eigenvalues, which is the main speed-bottleneck.
According to the idea of MMC, the optimization problem is defined as
\begin{equation}
	\begin{array}{c}
		\min_{\mathbf{M}\in\mathbb{S}^d_+}\sum_{(\mathbf{x_i},\mathbf{x_j})\in S}d_{\mathbf{M}}(\mathbf{x_i},\mathbf{x_j})\\
		\\
		\text{s.t.} \quad \sum_{(\mathbf{x_i},\mathbf{x_j})\in D}d_{\mathbf{M}}^2(\mathbf{x_i},\mathbf{x_j})\ge 1
	\end{array}
\end{equation}
where $\mathbf{M}$ is the Mahalanobis matrix and thus a positive semi-definite matrix ($\mathbf{M}\in\mathbb{S}^d_+$) and $d_{\mathbf{M}}(\cdot)$ is the Mahalanobis distance. As we can see, The algorithm aims at minimizing the sum of distances between all the similar points, while constrains the sum of distances between dissimilar points.
\subsubsection{RCA}
\par
Relative Components Analysis \cite{shental2002adjustment} is a kind of weakly supervised learning method. It learns a full rank Mahalanobis distance metric based on a weighted sum of in-chunklets covariance matrices. It applies a global linear transformation to assign large weights to relevant dimensions and low weights to irrelevant dimensions. 
Those relevant dimensions are estimated using “chunklets” which are subsets of points that are known to belong to the same class.
For a training set with $n$ training points in $k$ chunklets (classes), the algorithm is efficient since it simply amounts to computing
\begin{equation}
	\mathbf{C}=\dfrac{1}{n}\sum_{j=1}^k\sum_{i=1}^{n_j}(\mathbf{x}_{ji}-\mu_j)(\mathbf{x}_{ji}-\mu_j)^T
\end{equation}
where chunklet $j$ consists of $\{\mathbf{x}_{ji}\}^{n_j}_{i=1}$ with mean $\mu_j$. THe inverse of $C^{-1}$ is used as the Mahalanobis matrix.
\subsubsection{Large Margin Nearest Neighbor Metric Learning}
\par
Large Margin Nearest Neighbor Metric Learning (LMNN) \cite{weinberger2009distance} is a kind of supervised learning method. It learns a Mahalanobis distance metric in the $k$-NN classification setting. 
The learned metric attempts to keep close k-nearest neighbors from the same class, while keeping examples from different classes separated by a large margin. This algorithm makes no assumptions about the distribution of the data.
The distance is learned by solving the following optimization problem:
\begin{equation}
	\begin{array}{l}
		\min _{\mathbf{L}} \sum_{i, j} \eta_{i j}\left\|\mathbf{L}\left(\mathbf{x}_{\mathbf{i}}-\mathbf{x}_{\mathbf{j}}\right)\right\|^{2}+\\
		c \sum_{i, j, l} \eta_{i j}\left(1-y_{i j}\right)\left[1+\left\|\mathbf{L}\left(\mathbf{x}_{\mathbf{i}}-\mathbf{x}_{\mathbf{j}}\right)\right\|^{2}-\left\|\mathbf{L}\left(\mathbf{x}_{\mathbf{i}}-\mathbf{x}_{\mathbf{l}}\right)\right\|^{2}\right]_{+}	
	\end{array}
\end{equation}
where $\mathbf{x}_j$ is one of the $k$ nearest neighbors sharing the same label with $\mathbf{x}_i$, $\mathbf{x}_l$ are all the other instances within that region with different labels. Both $\eta_{ij}$ and $y_{ij}$ are indicators in $\{0,1\}$. $\eta_{ij}$ represents $\mathbf{x}_j$ is the $k$ nearest neighbors with same labels of $mathbf{x}_i$.
$y_{ij}=0$ indicates $\mathbf{x}_i,\mathbf{x}_j$ belong to different class. $[\cdot]_+=\max(0,\cdot)$ is the Hinge loss.
\section{Simple Distance Metrics}
\subsection{Minkowski Distance}
In this part, we use the Euclidean distance, the Chebyshev distance and the Manhattan distance on the $k$-NN with our dataset. We tried different $k$'s and the results differs with regard to different $k$ values. Here is the result:
\begin{table}[htbp]
	\centering
 	\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}
 	\renewcommand\arraystretch{1.0}
 	\caption{Comparison of $k-$NN Classification Task with Different Minkowski Distances}
 	\label{base0}%
 		\begin{tabular}{@{}p{1cm}<{\centering}|c|c|c}
 		\hline
 		\multirow{2}{*}{\diagbox[height=2\line,width=1.42cm,font=\tiny]{$k$}{Acc.}{$\mathit{M}$}} &
 		\multicolumn{3}{c}{$k-$NN with different Minkowski distances}\\
 		\cline{2-4}
			& Euclidean(\%) & Chebyshev(\%) & Manhattan(\%)\\
 		\hline
 		2   & 84.56 & 63.45 & 84.75 \\
 		3   & 86.89 & 66.64 & \textbf{86.62} \\
 		4   & 86.94 & 67.77 & 86.51 \\
 		5   & 87.44 & 68.44 & 86.52 \\
 		6   & 87.25 & 68.95 & 86.40 \\
 		7   & \textbf{87.61} & 69.07 & 86.60 \\
 		8   & 87.37 & 69.37 & 86.38 \\ 
 		9   & 87.54 & 69.35 & 86.38 \\
 		10   & 87.32 & 69.32 & 86.36 \\
 		11   & 87.35 & 69.37 & 86.28 \\
 		12   & 87.19 & 69.38 & 86.15 \\
 		13   & 87.16 & \textbf{69.44} & 85.97 \\
 		14   & 87.09 & 69.28 & 85.87 \\
 		15   & 86.10 & 69.31 & 85.79 \\
		16   & 86.84 & 69.16 & 85.54 \\
		50   & 84.46 & 65.99 & 82.44 \\
		100   & 82.06 & 62.61 & 79.36 \\
		200   & 78.14 & 58.50 & 74.30 \\
		500   & 70.53 & 51.28 & 64.93 \\
		1000   & 62.51 & 44.21 & 54.95 \\
		\hline
		\end{tabular}
\end{table}
\subsection{Dimension Reduction and Minkowski Distance}
In this part, we explored the effect of dimension reduction on $k$-NN with different Minkowski distances. We did experiments on different methods including PCA and LDA, different kernels of PCA and different dimensions.
Firstly, we tried different kernels (linear, rbf) of PCA and reduced the original 2048 dimensions into 50 dimensions.
\begin{table}[htbp]
	\centering
 	\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}
 	\renewcommand\arraystretch{1.0}
 	\caption{Comparison of Linear PCA+$k-$NN Classification Task with Different Minkowski Distances}
 	\label{base1}%
 		\begin{tabular}{@{}p{1cm}<{\centering}|c|c|c}
 		\hline
 		\multirow{2}{*}{\diagbox[height=2\line,width=1.42cm,font=\tiny]{$k$}{Acc.}{$\mathit{M}$}} &
 		\multicolumn{3}{c}{Linear PCA+$k-$NN with different Minkowski distances}\\
 		\cline{2-4}
			& Euclidean(\%) & Chebyshev(\%) & Manhattan(\%)\\
 		\hline
 		2   & 83.96 & 77.00 & 83.58 \\
 		3   & 86.34 & 79.54 & 85.91 \\
 		4   & 86.70 & 80.40 & 86.42 \\
 		5   & 87.17 & 80.88 & 87.03 \\
 		6   & 87.18 & 80.90 & 86.80 \\
 		7   & 87.41 & 81.24 & 87.02 \\
 		8   & 87.50 & 81.07 & 86.88 \\ 
 		9   & 87.45 & 81.25 & 87.07 \\
 		10   & 87.33 & 81.24 & \textbf{87.16} \\
 		11   & \textbf{87.51} & \textbf{81.29} & 87.13 \\
 		12   & 87.39 & 81.06 & 87.03 \\
 		13   & 87.50 & 81.13 & 87.03 \\
 		14   & 87.26 & 80.95 & 86.92 \\
 		15   & 87.23 & 80.97 & 86.94 \\
		16   & 86.92 & 80.99 & 86.83 \\
		50   & 85.20 & 78.92 & 84.87 \\
		100   & 83.21 & 76.25 & 83.06 \\
		200   & 80.35 & 72.71 & 80.48 \\
		500   & 75.60 & 67.08 & 75.51 \\
		1000   & 70.13 & 60.73 & 70.18 \\
		\hline
 		Baseline & \multicolumn{3}{c}{\textbf{87.61}} \\
		\hline
		\end{tabular}
\end{table}
\begin{table}[htbp]
	\centering
 	\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}
 	\renewcommand\arraystretch{1.0}
 	\caption{Comparison of RBF PCA+$k-$NN Classification Task with Different Minkowski Distances}
 	\label{base1_1}%
 		\begin{tabular}{@{}p{1cm}<{\centering}|c|c|c}
 		\hline
 		\multirow{2}{*}{\diagbox[height=2\line,width=1.42cm,font=\tiny]{$k$}{Acc.}{$\mathit{M}$}} &
 		\multicolumn{3}{c}{RBF PCA+$k-$NN with different Minkowski distances}\\
 		\cline{2-4}
			& Euclidean(\%) & Chebyshev(\%) & Manhattan(\%)\\
 		\hline
 		2   & 82.72 & 76.65 & 82.89 \\
 		3   & 85.45 & 79.34 & 85.71 \\
 		4   & 85.84 & 80.05 & 85.95 \\
 		5   & 86.29 & 80.76 & 86.49 \\
 		6   & 86.32 & 80.92 & 86.53 \\
 		7   & \textbf{86.74} & 81.28 & 86.75 \\
 		8   & 86.60 & 81.31 & 86.70 \\ 
 		9   & 86.57 & 81.37 & \textbf{86.84} \\
 		10   & 86.56 & 81.47 & 86.73 \\
 		11   & 86.47 & 81.39 & 86.78 \\
 		12   & 86.38 & 81.41 & 86.64 \\
 		13   & 86.40 & 81.55 & 86.68 \\
 		14   & 86.48 & 81.64 & 86.64 \\
 		15   & 86.36 & 81.59 & 86.55 \\
		16   & 86.49 & \textbf{81.67} & 86.63 \\
		50   & 84.53 & 79.49 & 84.57 \\
		100   & 82.61 & 77.42 & 83.18 \\
		200   & 80.16 & 74.69 & 80.49 \\
		500   & 75.54 & 69.49 & 75.66 \\
		1000   & 69.84 & 64.02 & 70.23 \\
		\hline
 		Baseline & \multicolumn{3}{c}{\textbf{87.61}} \\
		\hline
		\end{tabular}
\end{table}

\begin{table}[htbp]
	\centering
 	\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}
 	\renewcommand\arraystretch{1.0}
 	\caption{Comparison of $k-$NN Classification Task with Euclidean Distance in Different Dimensions}
 	\label{base1_2}%
 		\begin{tabular}{@{}p{1cm}<{\centering}|c|c|c}
 		\hline
 		\multirow{2}{*}{\diagbox[height=2\line,width=1.42cm,font=\tiny]{$k$}{Acc.}{$\mathit{M}$}} &
 		\multicolumn{3}{c}{$k-$NN with Euclidean distance in different dimensions}\\
 		\cline{2-4}
			& Linear,50(\%) & Linear,500(\%) & Original,2048(\%)\\
 		\hline
 		2   & 83.96 & \textbf{66.01} & 84.56 \\
 		3   & 86.34 & 64.72 & 86.89 \\
 		4   & 86.70 & 63.65 & 86.94 \\
 		5   & 87.17 & 61.72 & 87.44 \\
 		6   & 87.18 & 60.39 & 87.25 \\
 		7   & 87.41 & 59.01 & \textbf{87.61} \\
 		8   & 87.50 & 57.72 & 87.37 \\ 
 		9   & 87.45 & 56.57 & 87.54 \\
 		10   & 87.33 & 55.44 & 87.32 \\
 		11   & \textbf{87.51} & 54.30 & 87.35 \\
 		12   & 87.39 & 53.45 & 87.19 \\
 		13   & 87.50 & 52.20 & 87.16 \\
 		14   & 87.26 & 51.55 & 87.09 \\
 		15   & 87.23 & 50.55 & 86.10 \\
		16   & 86.92 & 49.45 & 86.84 \\
		50   & 85.20 & 32.90 & 84.46 \\
		100   & 83.21 & 22.63 & 82.06 \\
		200   & 80.35 & 13.87 & 78.14 \\
		500   & 75.60 & 06.67 & 70.53 \\
		1000   & 70.13 & 05.17 & 62.51 \\
		\hline
 		Baseline & \multicolumn{3}{c}{\textbf{87.61}} \\
		\hline
		\end{tabular}
\end{table}

\begin{table}[htbp]
	\centering
 	\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}
 	\renewcommand\arraystretch{1.0}
 	\caption{Comparison of $k-$NN Classification Task with Linear PCA and LDA}
 	\label{base1_3}%
 		\begin{tabular}{@{}p{1cm}<{\centering}|c|c}
 		\hline
 		\multirow{2}{*}{\diagbox[height=2\line,width=1.42cm,font=\tiny]{$k$}{Acc.}{$\mathit{M}$}} &
 		\multicolumn{2}{c}{$k-$NN with Linear PCA and LDA}\\
 		\cline{2-3}
			& PCA,50(\%) & LDA,40(\%) \\
 		\hline
 		2   & 83.96 & 88.21 \\
 		3   & 86.34 & 89.95 \\
 		4   & 86.70 & 89.91 \\
 		5   & 87.17 & 90.41 \\
 		6   & 87.18 & 90.26 \\
 		7   & 87.41 & 90.46 \\
 		8   & 87.50 & 90.55 \\ 
 		9   & 87.45 & 90.62 \\
 		10   & 87.33 & 90.58 \\
 		11   & \textbf{87.51} & 90.65 \\
 		12   & 87.39 & 90.70 \\
 		13   & 87.50 & 90.76 \\
 		14   & 87.26 & 90.68 \\
 		15   & 87.23 & \textbf{90.77} \\
		16   & 86.92 & 90.74 \\
		\hline
 		Baseline & \multicolumn{2}{c}{\textbf{87.61}} \\
		\hline
		\end{tabular}
\end{table}
\subsection{Cosine Distance}
\section{Metric Learning}
\subsection{Information Theoretic Metric Learning (ITML)}
% ITML is an information-besed learning approach taht formulate the problem as that of minimizing the differential relative entropy between two multivariate Gaussians under constraints on the distance function\cite{Davis2007Information}. In this section, we will introduce the experiment on pure ITML, PCA-preprocessed ITML and LDA-preprocessed ITML.

In this experiment, we trained supervised ITML model in training dataset, then transform source data to ITML-learned data for KNN classification. We set $constraints=200$ in supervised ITML model and give the $k-$neighbors range in $k-$NN model as $[2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]$. For PCA-preprocessed ITML, we first perprocess the data by 50-conponents linear PCA, then train supervised ITML model on the processed data, it will give a better performance. For LDA-preprocessed one, we perprocess the data by 40-conponents LDA, instead.

\begin{table}[htbp]
	\centering
 	\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}
 	\renewcommand\arraystretch{1.0}
 	\caption{Comparison of Variance ITML and baselines in $k-$NN Classification Task}
 	\label{base2}%
 		\begin{tabular}{@{}p{1cm}<{\centering}|c|c|c}
 		\hline
 		\multirow{2}{*}{\diagbox[height=2\line,width=1.42cm,font=\tiny]{$k$}{Acc.}{$\mathit{M}$}} &
 		\multicolumn{3}{c}{Variance ITML model + $k-$NN}\\
 		\cline{2-4}
 		& {ITML(\%)} & {PCA-preprocessed(\%)} & {LDA-preprocessed(\%)}\\
 		\hline
 		2   & 82.04 & 83.18 & 88.06\\
 		3   & \textbf{83.60} & 85.41 & 89.57\\
 		4   & 83.31 & 85.55 & 89.42\\
 		5   & 83.52  & 86.19 & 90.01\\
 		6   & 83.33  & 86.21 & 90.04\\
 		7   & 83.37  & \textbf{86.67} & 90.35\\
 		8   & 83.02  & 86.51 & 90.35\\
 		9   & 82.97  & 86.37 & 90.40\\
 		10   & 82.72  & 86.41 & 90.55\\
 		11   & 82.73  & 86.25 & 90.64\\
 		12   & 82.52  & 86.27 & 90.67\\
 		13   & 82.47  & 86.17 & 90.58\\
 		14   & 82.33  & 86.06 & 90.64\\
 		15   & 82.10  & 86.15 & \textbf{90.74}\\
		16   & 81.96  & 86.01 & 90.69\\
		\hline
 		Baseline & \multicolumn{3}{c}{\textbf{87.61}} \\
 		\hline
 	\end{tabular}
\end{table}

Our experiment results are shown in Tab. \ref{base2}. Compared with baseline $87.61\%$ in Euclidean KNN, we can find that simple ITML model and PCA-preprocessed ITML model have poor performance, we deem the reason is that ITML minimize the LogDet divergence subject to linear constraints while it does not rely on an eigenvalue computation, and PCA can't differ the classes which is useless for $k-$NN classification. But we can find PCA-preprocessed model have better performance than pure ITML model, we think PCA method can have compensation on eigenvalue computation for ITML. When we set $k=15$, we have best performance $90.74\%$, we think LDA method can make the variance high enough between different clusters. 


\subsection{Local Fisher Discriminant Analysis (LFDA)}

% Local Fisher Discriminant Analysis (LFDA) \cite{Sugiyama2008Semi} is a linear supervised dimensionality reduction method while ITML tends to be information-based method. LFDA extends LDA by assigning greater weights to those connecting examples that are nearby rather than distant. It is particularly useful when dealing with multimodality, where one ore more classes consist of separate clusters in input space. In this section, we will introduce the experiment on pure LFDA, PCA-preprocessed LFDA and LDA-preprocessed LFDA.

In this experiment, we trained LFDA model in training dataset, then project source data to LFDA-learned data for KNN classification. We set $k-$neighbors range in $k-$NN model as $[2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]$. For PCA-preprocessed LFDA, we first perprocess the data by 50-conponents linear PCA, then train supervised ITML model on the processed data, it will give a better performance. For LDA-preprocessed one, we perprocess the data by 40-conponents LDA, instead.

\begin{table}[htbp]
	\centering
 	\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}
 	\renewcommand\arraystretch{1.0}
 	\caption{Comparison of Variance LFDA and baselines in $k-$NN Classification Task}
 	\label{base3}%
 		\begin{tabular}{@{}p{1cm}<{\centering}|c|c}
 		\hline
 		\multirow{2}{*}{\diagbox[height=2\line,width=1.42cm,font=\tiny]{$k$}{Acc.}{$\mathit{M}$}} &
 		\multicolumn{2}{c}{Variance LFDA model + $k-$NN}\\
 		\cline{2-3}
 		& {PCA-preprocessed(\%)} & {LDA-preprocessed(\%)}\\
 		\hline
 		2   & 84.00 & 88.18\\
 		3   & 86.30 & 89.95\\
 		4   & 86.74 & 89.94\\
 		5   & 87.17 & 90.38\\
 		6   & 87.16 & 90.29\\
 		7   & 87.40 & 90.43\\
 		8   & 87.42 & 90.54\\
 		9   & 87.51 & 90.62\\
 		10   & 87.46 & 90.55\\
 		11   & \textbf{87.51} & 90.63\\
 		12   & 87.38 & 90.68\\
 		13   & 87.49 & 90.71\\
 		14   & 87.24 & 90.68\\
 		15   & 87.23 & \textbf{90.76}\\
		16   & 86.97 & 90.76\\
		\hline
 		Baseline & \multicolumn{2}{c}{\textbf{87.61}} \\
 		\hline
 	\end{tabular}
\end{table}

Our experiment results are shown in Tab. \ref{base3}. We find simple LFDA model have poor performance as $16.20\%$, we think this may be caused by that we set the n\_components parameter as default, and it means we can't control its dimensionality reduction process to get a satisfied result. Compared with baseline $87.61\%$ in Euclidean KNN, we find LDA-preprocessed model reached $90.76\%$ while PCA-preprocessed one reached $87.51\%$, we deem the reason is that LDA can make the variance high enough between different clusters.

\subsection{Mahalanobis Metric Learning for Clustering (MMC)}
In this experiment, we trained MMC model in training dataset, then project source data to MMC-learned data for KNN classification. We set $k-$neighbors range in $k-$NN model as $[2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]$. For PCA-preprocessed LFDA, we first perprocess the data by 50-conponents linear PCA, then train MMC model on the processed data, it will give a better performance. For LDA-preprocessed one, we perprocess the data by 40-conponents LDA, instead.
\begin{table}[htbp]
	\centering
 	\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}
 	\renewcommand\arraystretch{1.0}
 	\caption{Comparison of Variance MMC and baselines in $k-$NN Classification Task}
 	\label{base4}%
 		\begin{tabular}{@{}p{1cm}<{\centering}|c|c|c}
 		\hline
 		\multirow{2}{*}{\diagbox[height=2\line,width=1.42cm,font=\tiny]{$k$}{Acc.}{$\mathit{M}$}} &
 		\multicolumn{3}{c}{Variance MMC model + $k-$NN}\\
 		\cline{2-4}
			& MMC(\%) & {PCA-preprocessed(\%)} & {LDA-preprocessed(\%)}\\
 		\hline
 		2   &   84.89&  83.73&87.39 \\
 		3   &   87.29&  86.48&89.38 \\
 		4   &   87.38&  86.35&89.47 \\
 		5   &   87.66&  87.02&89.80 \\
 		6   &   87.58&  86.79&89.91 \\
 		7   & \textbf{87.82}  & 87.22 & 90.05\\
 		8   &   87.72&  87.01&90.04 \\
 		9   &   87.46& \textbf{97.23} &90.16 \\
 		10   &   87.46&  87.12&90.07 \\
 		11   &   87.53&  87.09& 90.24\\
 		12   &   87.48&  87.05&90.24 \\
 		13   &   87.60&  86.95& 90.26\\
 		14   &   87.44&  87.02& 90.30\\
 		15   &   87.49&  86.99& \textbf{90.34}\\
		16   &   87.27&  86.97&90.28 \\  
		\hline
		Baseline & \multicolumn{3}{c}{\textbf{87.61}} \\
 		\hline
		\end{tabular}
\end{table}

Our experiment results are shown in Tab. \ref{base4}. We find PCA-preprocessed MMC reached $97.23\%$, while simple MMC reached $87.82\%$ and LDA-preprecessed MMC reached $90.34\%$. We speculate that this is due to the reduction of overfitting after dimensionality reduction. In addition, the idea of LDA is slightly similar to that of MMC, both of which are to close the data of the same class and push away the data of different classes. It may be that the data has been processed twice with similar effects, resulting in experimental performance that is not as good as PCA-preprocessed MMC.
\subsection{Relevant Component Analysis (RCA)}
In this experiment, we trained MMC model in preprocessed training dataset for lacking of computing resourses, then project source data to MMC-learned data for KNN classification. We set $k-$neighbors range in $k-$NN model as $[2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]$. For PCA-preprocessed RCA, we first perprocess the data by 50-conponents linear PCA, then train RCA model on the processed data. For LDA-preprocessed one, we perprocess the data by 40-conponents LDA, instead.
\begin{table}[htbp]
	\centering
 	\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}
 	\renewcommand\arraystretch{1.0}
 	\caption{Comparison of Variance RCA and baselines in $k-$NN Classification Task}
 	\label{base5}%
 		\begin{tabular}{@{}p{1cm}<{\centering}|c|c}
 		\hline
 		\multirow{2}{*}{\diagbox[height=2\line,width=1.42cm,font=\tiny]{$k$}{Acc.}{$\mathit{M}$}} &
 		\multicolumn{2}{c}{Variance RCA model + $k-$NN}\\
 		\cline{2-3}
		    & {PCA-preprocessed(\%)} & {LDA-preprocessed(\%)}\\
 		\hline
 		2   &82.65  &87.39 \\
 		3   &85.15  &89.38 \\
 		4   &85.37  &89.47 \\
 		5   &85.83  &89.80 \\
 		6   &86.07  &89.91 \\
 		7   &\textbf{86.30} &90.05 \\
 		8   &86.14  &90.04 \\
 		9   &86.12  &90.16 \\
 		10   &85.96  &90.07 \\
 		11   &86.03  &90.24 \\
 		12   &86.09  &90.24 \\
 		13   &86.09  &90.26  \\
 		14   &85.90  &90.30 \\
 		15   &85.83  &\textbf{90.34} \\
		16   &85.73  &90.28 \\
		\hline
		Baseline & \multicolumn{2}{c}{\textbf{87.61}} \\
 		\hline
 	\end{tabular}
\end{table}

Our experiment results are shown in Tab. \ref{base5}. We find LDA-preprocessed RCA which reached $90.34\%$ performed better than PCA-preprocessed RCA which reached $86.30\%$. RCA uses the information in such chunklets to reduce irrelevant variability in the data while amplifying relevant variability. Therefore, we deem the reason is that PCA lacks attention to categories when reducing dimensionality.
\subsection{Large Margin Nearest Neighbor (LMNN)}

In this experiment, we trained LMNN model in PCA-preprocessed training dataset and LDA-preprocessed training dataset, then project the data to LMNN-learned data for KNN classification. Due to lack of computing resources, we did not use the original dataset. LMNN needs to set the target neighbor k, so we also set different target neighbors for experiment. We set $K-$neighbors in KNN range as $[2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]$. For PCA-preprocessed LFDA, we set $k-$target neighbors in LMNN range as $[2,3,4,5,6]$. We first perprocess the data by 50-conponents linear PCA, then train LMNN model on the processed data. For LDA-preprocessed one, we perprocess the data by 40-conponents LDA instead and set $k-$target neighbors in LMNN range as $[2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]$. 
\begin{table}[htbp]
	\centering
 	\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}
 	\renewcommand\arraystretch{1.0}
 	\caption{Comparison of Variance PCA+LMNN and baselines in $k-$NN Classification Task}
 	\label{base6}%
 		\begin{tabular}{@{}p{1cm}<{\centering}|c|c|c|c|c}
 		\hline
 		\multirow{2}{*}{\diagbox[height=2\line,width=1.42cm,font=\tiny]{$k$}{Acc.}{$\mathit{M}$}} &
 		\multicolumn{5}{c}{Variance PCA+LMNN model + $k-$NN}\\
 		\cline{2-6}
		    & {k=2(\%)} & {k=3(\%)} & {k=4(\%)} & {k=5(\%)} & {k=6(\%)}\\
 		\hline
 		2   &84.65  &84.66  &84.55  &84.79  &84.84  \\
 		3   &87.01  &87.19  &86.98  &87.05  &87.11  \\
 		4   &87.37  &87.42  &87.48  &87.24  &87.27  \\
 		5   &87.80  &87.83  &87.87  &87.98  &87.70  \\
 		6   &87.73  &87.80  &87.76  &87.90  &87.82  \\
 		7   &88.07  &88.05  &88.09  & \textbf{88.21} &88.19  \\
 		8   &87.99  &87.95  &88.05  &88.09  &88.03  \\
 		9   &88.18  & \textbf{88.13} &88.21  &88.16  &88.19  \\
 		10   &87.99  &88.06  &88.10  &88.05  &88.19  \\
 		11   & \textbf{88.18} &88.04  &88.12  &88.18  &88.13  \\
 		12   &88.07  &88.01  &88.07  &88.02  &88.13  \\
 		13   &88.03  &88.03  & \textbf{88.24} &88.15  &88.21  \\
 		14   &97.90  &97.90  &87.96  &88.09  &88.19  \\
 		15   &87.90  &88.00  &88.10  &88.10  & \textbf{88.23} \\
		16   &87.80  &87.90  &88.05  &88.03  &88.21  \\  
		\hline
		Baseline & \multicolumn{5}{c}{\textbf{87.61}} \\
 		\hline
 	\end{tabular}
\end{table}

\begin{table}[htbp]
	\centering
 	\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}
 	\renewcommand\arraystretch{1.0}
 	\caption{Comparison of Variance LDA+LMNN and baselines in $k-$NN Classification Task}
 	\label{base7}%
 		\begin{tabular}{@{}p{1cm}<{\centering}|c|c|c|c|c|c}
 		\hline
 		\multirow{2}{*}{\diagbox[height=2\line,width=1.42cm,font=\tiny]{$k$}{Acc.}{$\mathit{M}$}} &
 		\multicolumn{6}{c}{Variance LDA+LMNN model + $k-$NN}\\
 		\cline{2-7}
		    & {k=2(\%)} & {k=3(\%)} & {k=4(\%)} & {k=5(\%)} & {k=6(\%)} & {k=7(\%)}\\
 		\hline
 		2   &88.37  &88.52  &88.33  &88.37  &88.29  &88.41  \\
 		3   &89.84  &90.12  &90.06  &89.92  &89.92  &89.83  \\
 		4   &89.79  &89.95  &90.05  &90.07  &90.03  &89.93  \\
 		5   &90.37  &90.46  &90.54  &90.50  &90.41  &90.44  \\
 		6   &90.07  &90.44  &90.33  &90.20  &90.27  &90.30  \\
 		7   &90.46  &90.62  &90.68  &90.73  &90.72  &90.68  \\
 		8   &90.53  &90.64  &90.58  &90.58  &90.68  &90.59  \\
 		9   &90.61  &90.74  &90.66  &90.72  &90.82  &90.73  \\
 		10   &90.56  &90.71  &90.68  &90.72  &90.69  &90.66  \\
 		11   & \textbf{90.77} &90.77  & \textbf{90.82} &90.82  &90.91  &90.85  \\
 		12   &90.64  &90.70  &90.75  &90.76  &90.78  &90.87  \\
 		13   &90.77  & \textbf{90.87} &90.80  &90.83  &90.82  & \textbf{90.89} \\
 		14   &90.72  &90.82  &90.77  &90.80  &90.76  &90.85  \\
 		15   &90.71  &90.84  &90.73  &90.83  & \textbf{90.99} &90.81  \\
		16   &90.71  &90.86  &90.82  & \textbf{90.88} &90.94  &90.86  \\  
		\hline
		Baseline & \multicolumn{6}{c}{\textbf{87.61}} \\
 		\hline
 	\end{tabular}
\end{table}

\begin{table}[htbp]
	\centering
 	\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}
 	\renewcommand\arraystretch{1.0}
 	\caption{Comparison of Variance LDA+LMNN and baselines in $k-$NN Classification Task (Continue)}
 	\label{base8}%
 		\begin{tabular}{@{}p{1cm}<{\centering}|c|c|c|c|c|c}
 		\hline
 		\multirow{2}{*}{\diagbox[height=2\line,width=1.42cm,font=\tiny]{$k$}{Acc.}{$\mathit{M}$}} &
 		\multicolumn{6}{c}{Variance LDA+LMNN model + $k-$NN}\\
 		\cline{2-7}
		    & {k=8(\%)} & {k=9(\%)} & {k=10(\%)} & {k=11(\%)} & {k=12(\%)} & {k=13(\%)}\\
 		\hline
 		2   &88.31  &88.26  &88.30  &88.21  &88.29  &88.33  \\
 		3   &89.87  &90.02  &89.92  &89.91  &90.00  &89.96  \\
 		4   &90.02  &90.09  &90.06  &89.99  &89.97  &89.97  \\
 		5   &90.54  &90.48  &90.58  &90.46  &90.48  &90.43  \\
 		6   &90.34  &90.36  &90.35  &90.29  &90.32  &90.37  \\
 		7   &90.70  &90.60  &90.64  &90.61  &90.60  &90.61  \\
 		8   &90.60  &90.50  &90.48  &90.48  &90.57  &90.54  \\
 		9   &90.77  &90.76  &90.70  &90.74  &90.64  &90.83  \\
 		10   &90.63  &90.63  &90.69  &90.61  &90.67  &90.65  \\
 		11   &90.74  &90.78  &90.78  &90.75  &90.82  &90.81  \\
 		12   &90.86  &90.82  &90.80  &90.78  &90.80  &90.79  \\
 		13   & \textbf{90.87} &90.85  &90.86  & \textbf{90.84} &90.90  &90.83  \\
 		14   &90.87  & \textbf{90.93} & \textbf{90.93} &90.82  &90.82  &90.83  \\
 		15   &90.86  &90.88  &90.89  &90.84  & \textbf{90.91} &90.89  \\
		16   &90.86  &90.88  &90.93  &90.78  &90.89  & \textbf{90.91} \\  
		\hline
		Baseline & \multicolumn{6}{c}{\textbf{87.61}} \\
 		\hline
 	\end{tabular}
\end{table}

\begin{table}[htbp]
	\centering
 	\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}
 	\renewcommand\arraystretch{1.0}
 	\caption{Comparison of Variance LDA+LMNN and baselines in $k-$NN Classification Task (Continue)}
 	\label{base9}%
 		\begin{tabular}{@{}p{1cm}<{\centering}|c|c|c}
 		\hline
 		\multirow{2}{*}{\diagbox[height=2\line,width=1.42cm,font=\tiny]{$k$}{Acc.}{$\mathit{M}$}} &
 		\multicolumn{3}{c}{Variance LDA+LMNN model + $k-$NN}\\
 		\cline{2-4}
			& {k=14(\%)} & {k=15(\%)} & {k=16(\%)}\\
 		\hline
 		2   &88.30   &88.31  &88.21 \\
 		3   &90.01   &90.06  &89.92 \\
 		4   &90.07   &90.09  &89.97 \\
 		5   &90.42   &90.39  &90.43 \\
 		6   &90.28   &90.23  &90.32 \\
 		7   &90.54   &90.54  &90.51 \\
 		8   &90.54   &90.50  &90.56 \\
 		9   &90.70   &90.68  &90.66 \\
 		10   &90.68   &90.67  &90.69 \\
 		11   &90.78   &90.72  &90.69 \\
 		12   &90.76   &90.79  &90.69 \\
 		13   &90.78   &90.85  &90.76 \\
 		14   &90.79   &90.82  &90.82 \\
 		15   &90.83   & \textbf{90.97} &90.83 \\
		16   & \textbf{90.87}  &90.93  & \textbf{90.89}\\  
		\hline
		Baseline & \multicolumn{3}{c}{\textbf{87.61}} \\
 		\hline
		\end{tabular}
\end{table}
Our experiment results are shown in Tab. \ref{base6}, Tab. \ref{base7}, Tab. \ref{base8}, Tab. \ref{base9}. We find both PCA-precessed LMNN and LDA-precessed LMNN have improved the performance of KNN. We think the main reason is that LMNN reduces the distance between similar samples and expands the distance between different samples. Compared with PCA-preprocessed LMNN which reached $88.24\%$, LDA-preprocessed LMNN performed better which reached $90.99\%$. We deem the reasion is that the idea of LDA is to minimize the intra-class variance and maximum inter-class variance after projection, which is helpful for classification. But we only set the $k-$target neighbors in LMNN change between 2-6 for PCA-preprocessed LMNN, this factor may also affect the experimental results. Another interesting finding is that the best experimental result is not achieved when $k = K$. We speculate that $k-$target neighbors represent our degree of supervision of the model. But we must also consider the amount of data for each class. Bcause when the number of certain types of data is less than k, there is not enough correct target neighbors to calculate the loss function, which will have a bad impact on the results. Therefore, the relationship between k and K for best results involves the trade-off between the data set itself and the intensity of supervision. We believe that if the parameter of $k-$target neighbors in LMNN can change according to each type in the dataset instead of the initial setting, it will play an optimizing role for LMNN.
\begin{center}
\begin{figure}
\centering
\subfigure[2D-scattering]{
\includegraphics[width=3.75cm]{pic/X_test_scatter.png}
%\caption{fig1}
}
\quad
\subfigure[PCA 2D-scattering]{
\includegraphics[width=3.75cm]{pic/X_test_scatter_PCA.png}
%\caption{fig1}
}
\quad
\subfigure[LDA 2D-scattering]{
\includegraphics[width=3.75cm]{pic/X_test_scatter_LDA.png}
}
\quad
\subfigure[PCA + LMNN 2D-scattering]{
\includegraphics[width=3.75cm]{pic/X_test_scatter_PCA_LMNN.png}
}
\quad
\subfigure[LDA + LMNN 2D-scattering]{
\includegraphics[width=3.75cm]{pic/X_test_scatter_LDA_LMNN.png}
}
\quad
\subfigure[PCA + MMC 2D-scattering]{
\includegraphics[width=3.75cm]{pic/X_test_scatter_PCA_MMC.png}
}
\quad
\subfigure[LDA + MMC 2D-scattering]{
\includegraphics[width=3.75cm]{pic/X_test_scatter_LDA_MMC.png}
}
\caption{tSNE Visualization with Variance Model}
\label{Fig2}
\end{figure}
\end{center}

\section{Conclusion}

In this project, we introduced traditional distance metric and several metric learning method, which is to learn a distance metric for the input space of data from a given collection of pair of similar/dissimilar points that preserves the distance relation among the training data. We have several simple distance matrics experiments like Minkowski, Chebyshev, Euclidean, Manhattan, Cosine and Mahalanobis, besides, we have several supervised experiments such as ITML, LFDA, MMC, RCA and LMNN, and give a experiment with unsupervised method PCA. In this section, we will conclude the pros and cons of every method and evaluate the performance of simple distance metrics and metrics learning models.

% PCA is a supervised dimensionality reduction method, which can preserve the global structure and maintain the variance of the data. In our project, we use PCA to processed data, i.e. we project the data into processed data, then we can use different distance metrics in $k-$NN, such as Euclidean, Manhattan, Chebyshev and Cosine. Most importantly, we can use it to preprocess data before we use anothor metircs learning method, we use this combination method in other experiments, for example, PCA-preprocessed ITML, it means we use PCA to proprecess the data, and then use ITML to finish metrics learning before $k-$NN steps.

% ITML is an information-besed learning approach that formulate the problem as that of minimizing the differential relative entropy between two multivariate Gaussians under constraints on the distance function\cite{Davis2007Information}. It does not rely on an eigenvalue computation but probability distributions, it results its performance can increase when we combine it with PCA-processed or LDA-processed.

% LFDA is a linear supervised dimensionality reduction method while ITML tends to be information-based method. It extend LDA by assigning greater weights to closer connecting examples and can only maintain the local data structure. In our experiment, we combine LFDA with PCA-processed or LDA-processed method to get a satisfied result.

% MMC minimizes the sum of squared distances between similar points, while enforcing the sum of distances between dissimilar ones to be greater than one. This leads to a convex and, thus, local-minima-free optimization problem that can be solved efficiently. In our experiment, we set 200 constrains for MMC evaluation, and find its satisfied performance.

% RCA learns a global linear transformation from the equivalence constraints, and the learned linear transformation can be used directly to compute distance between any two examples. It can preserve global structure and all the positive/negative constrains, it means it will cost very large memory and take too long, therefore in our experiment we use PCA and LDA to preprocess the source data instead of evaluating pure RCA model.

% LMNN is a linear model that extends NCA through a maximum margin framework. It learns a Mahalanobis distance that tries to collapse examples in the same class to a single point, and in the meantime keep examples from different classes far away. LMNN preserve the local information as NCA but have better performance than NCA. In our experiments, to prevent memory explosion, we only evaluate performance of LMNN model at $k-$neighbors range of $[2,3,4,5,6]$ and use PCA or LDA to preprocess the data.

For simple metrics, we can find different methods should be applied to different data structures, which should be discussed in detail. Metrics learning methods can combined with simple metrics and usually get better results. For metrics learning methods, we find if preprocessed method is not used, LFDA's result is not satisfied, we deem the reason that the $n\_compoents$ parameter in FLDA is not set but use default one. In order to save the memory consumption of training model in RCA and LMNN, preprocess method must be attached, and anothor reason is that with PCA-preprocessed method or LDA-preprocessed method, we can get better result. In Tab. \ref*{final1}, we show performance comprision of all the metrics learning methods and we will analyse it as follows.

As is shown in Fig. \ref*{Fig2}, we can find LDA can maintain the distance better between different cluster than PCA, it can explain why LDA-preprocessed method usually have better performance than PCA-preprocessed one, for instance, (d)(e) can subscribe the reason why $k-$NN with LDA-preprocessed method achieve higher accuracy in LMNN. (f)(g) may contradict the experimental results that PCA-preprocessed MMC have better performance than LDA-preprocessed one, we deem the reason that when we set $n\_components=50$ in PCA, it performs well but tSNE can't show this property.

\begin{table}[htbp]
	\centering
 	\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}
 	\renewcommand\arraystretch{1.0}
 	\caption{Comparison of Metrics Learning and baselines in $k-$NN Classification Task}
 	\label{final1}%
 		\begin{tabular}{@{}p{3cm}<{\centering}|c|c}
 		\hline
 		\multirow{2}{*}{\diagbox[height=2\line,width=3.42cm,font=\tiny]{$M$}{preM+Acc.}{$\mathit{}$}} &
 		\multicolumn{2}{c}{Variance metrics learning model + $k-$NN}\\
 		\cline{2-3}
 		& {preprocess model(\%)} & {best performance(\%)}\\
 		\hline
 		ITML   & LDA & 90.74\\
 		LFDA   & LDA & 90.76\\
 		MMC   & PCA & 97.23\\
 		RCA   & LDA & 90.34\\
 		LMNN   & LDA & 90.99\\
		\hline
 		Baseline & \multicolumn{2}{c}{\textbf{87.61}} \\
 		\hline
 	\end{tabular}
\end{table}


\bibliography{Prj2}
\end{document}
